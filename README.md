# The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks
This is the repository for the project: The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks, in which we examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total.
Our results show that:

i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently;

ii) datasets generated by humans align better with each other than with synthetic datasets, or than the latter among themselves;

iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compositionality;

iv) specific lexical items in dataset impacts the measurement consistency.

You can find the original repositories of the models that we ran here: [COGS](https://github.com/najoungkim/COGS), [TMCD](https://github.com/google-research/language/tree/master/language/compgen/nqg), [Seq2seq on COGS](https://github.com/coli-saar/Seq2seq-on-COGS), [Neural-BTG](https://github.com/berlino/btg-seq2seq/tree/master/neural_btg).

## Environment

First, create a virtual environment with `environment.yml` and `export BASE_DIR=/path/to/this/dir`. 
By default, the environment will be named as `compgen`, which will continued to be used in the script below. 

Data will be stored in the path of `$BASE_DIR/data` and trained model files will be stored in `$BASE_DIR/trained_models` for each dataset split.

## üå≤ Data Preparation

### Download the data
Since the datasets have different license, they should be downloaded from the original sources listed below and use the scripts in this repository for pre-processing.

| **Dataset** | **Source for Download**                                                                                                              |
|-------------|--------------------------------------------------------------------------------------------------------------------------------------|
| COGS        | https://github.com/najoungkim/COGS                                                                                                   |
| SCAN        | https://github.com/brendenlake/SCAN                                                                                                  |
| SCAN-MCD    | https://github.com/google-research/google-research/tree/master/cfq#scan-mcd-splits                                                   |
| GEOQUERY    | http://www.cs.utexas.edu/~ml/wasp/geo-funql/corpus.xml, and ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase for GEOBASE |
| SPIDER      | https://yale-lily.github.io/spider                                                                                                   |


**üì† SCAN**

Use the `baseline_replication/TMCD/tasks/scan/convert_to_tsv.py` to convert the original SCAN format into TSV files, which we'll use to train all the models.
Use `baseline_replication/TMCD/tasks/scan/join_txt_to_tsv.py` to join the input and output txt files generated for the MCD splits.

**üåè GEOQUERY and üï∑Ô∏è SPIDER**

Follow the instruction in the [repository of NQG](https://github.com/google-research/language/tree/master/language/compgen/nqg) to come through the pre-processing.

**ü™µ COGS**

Simply download the data in the COGS source, and put it in the data dir.

When data are successfully gathered, the `data/` directory should have structure like below:

    data
    |--geoquery/
        |--standard_split/
            |--train.tsv
            |--test.tsv
        |--template_split/
        |--length_split/
        |--tmcd_split/
    |--spider
        |--tables.json
        |--standard_split/
        |--template_split/
        |--length_split/
        |--tmcd_split/
    |--SCAN
        |--standard_split/
        |--template_split/
        |--length_split/
        |--tmcd_split/
    |--COGS
        |--standard_split/

### Generate Lexical Heldout and Length split
To generate lexically modified splits, use the command below, with `new_heldou_type = {'random_str', 'random_cvcv'}`.
```
python reformat_lexical_heldouts.py --dataset 'SCAN' \
    --split 'turn_left' \
    --input_path 'data/SCAN/addprim_turn_left_split/' \
    --output_path 'data/SCAN/turn_left_random_str/' \
    --new_heldout_type 'random_str'
```

## üñ•Ô∏è Training

LSTM, Transformer, T5, BART, and Neural-BTG are used for training. This section provide the instruction to train these models.

### LSTM & Transformer
Use the script `train_lstm.sh` and `train_transformer.sh` under `exp_script/` to train LSTM and Transformer.

Modify the `dataset` and `split` parameters on top of each script to specify the desired dataset split.

### T5
For T5 training on COGS, use `train_t5_cogs.sh`, which contains the trainig strategy from Orhan, 2021 to train T5 on COGS.

For the other dataset, use `train_hf.sh` to train and evaluate the model.

### Neural-BTG
Use `train_btg.sh` to train Neural-BTG.


## üìà Evaluation
After the training of each model, the script should automatically output predictions to the folder `preds/`. 
Use the functions in `utils/evaluate_utils.py` to evaluate each model.

This script allows multiple variants of exact match accuracy, and has functions to evluate only on one model variant, evaluate on all random seeds, evaluate all datasets of a model, evaluate all model for a single dataset, and evaluate everything used in our work.

By default, calling `python utils/evalaute_utils.py` will evaluate all models on each dataset split, output the performance of each random seed into `results/exact_match.csv`, and output the average performance into `results/perf_table.csv`.

## üìä Figure and Analysis
The code to reproduce each experiment in our work can be found in `utils/gen_plot_for_sub.py` with documentation of each function.

## ü´∂ Citation
```
Kaiser Sun, Adina Williams, Dieuwke Hupkes. The validity of evaluation results: assessing concurrence across compositionality benchmarks. CoNLL 2023.
```
